{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "#NLTK\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import words\n",
    "\n",
    "#spaCy\n",
    "import spacy\n",
    "\n",
    "#SKlearn\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>authors</th>\n",
       "      <th>publisher</th>\n",
       "      <th>blurb</th>\n",
       "      <th>full_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Generación idiota: Una crítica al adolescentrismo</td>\n",
       "      <td>Agustin Laje</td>\n",
       "      <td>HarperCollins Mexico</td>\n",
       "      <td>El controversial escritor Agustín Laje adviert...</td>\n",
       "      <td>Generación idiota: Una crítica al adolescentri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Spare: En la sombra</td>\n",
       "      <td>Prince Harry The Duke of Sussex</td>\n",
       "      <td>PRH Grupo Editorial</td>\n",
       "      <td>La controversial autobiografía del príncipe Ha...</td>\n",
       "      <td>Spare: En la sombra PRH Grupo Editorial La con...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Volver a empezar / It Starts with Us (Spanish ...</td>\n",
       "      <td>Colleen Hoover</td>\n",
       "      <td>Planeta Publishing Corporation</td>\n",
       "      <td>¡Colleen Hoover nos brinda su magia nuevamente...</td>\n",
       "      <td>Volver a empezar / It Starts with Us (Spanish ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>¡Vámonos a la Estufa! con Janet Jauja Cocina M...</td>\n",
       "      <td>Kushner Janet</td>\n",
       "      <td>Larousse</td>\n",
       "      <td>Jauja, la popular chef estrella de YouTube, no...</td>\n",
       "      <td>¡Vámonos a la Estufa! con Janet Jauja Cocina M...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RUGE: O espera a ser devorado (Spanish Edition)</td>\n",
       "      <td>Daniel Habif</td>\n",
       "      <td>Editorial Planeta Mexicana S.A. de C.V.</td>\n",
       "      <td>Con su clásico agresivo pero muy alentador est...</td>\n",
       "      <td>RUGE: O espera a ser devorado (Spanish Edition...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>977</th>\n",
       "      <td>EL DR. SILKWORTH EN ALCOHÓLICOS ANÓNIMOS</td>\n",
       "      <td>Oslos Molina Oslos Molina</td>\n",
       "      <td>Experiencias AA</td>\n",
       "      <td>Bill decía frecuentemente que el programa de A...</td>\n",
       "      <td>EL DR. SILKWORTH EN ALCOHÓLICOS ANÓNIMOS Exper...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>978</th>\n",
       "      <td>La Inspiración de Rumi: 100 Citas Para Elevar ...</td>\n",
       "      <td>David Smith</td>\n",
       "      <td>Independently published</td>\n",
       "      <td>En este libro, te sumergirás en la sabiduría y...</td>\n",
       "      <td>La Inspiración de Rumi: 100 Citas Para Elevar ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>979</th>\n",
       "      <td>El Derecho como objeto de investigación: Enfoq...</td>\n",
       "      <td>Elmer Arce Ortíz</td>\n",
       "      <td>Palestra Editores</td>\n",
       "      <td>Este libro es una presentación de la investiga...</td>\n",
       "      <td>El Derecho como objeto de investigación: Enfoq...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>980</th>\n",
       "      <td>Reflexiones jurídicas sobre la Jurisdicción Es...</td>\n",
       "      <td>Jaime Bernal Cuéllar</td>\n",
       "      <td>Universidad Externado</td>\n",
       "      <td>El libro Reflexiones jurídicas sobre la Jurisd...</td>\n",
       "      <td>Reflexiones jurídicas sobre la Jurisdicción Es...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>981</th>\n",
       "      <td>El arte de no encajar: Lo que pasa cuando el a...</td>\n",
       "      <td>Noemí Navarro noemimisma</td>\n",
       "      <td>Penguin Random House Grupo Editorial España</td>\n",
       "      <td>El libro testimonial de @Noemimisma Este es el...</td>\n",
       "      <td>El arte de no encajar: Lo que pasa cuando el a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>982 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 title  \\\n",
       "0    Generación idiota: Una crítica al adolescentrismo   \n",
       "1                                  Spare: En la sombra   \n",
       "2    Volver a empezar / It Starts with Us (Spanish ...   \n",
       "3    ¡Vámonos a la Estufa! con Janet Jauja Cocina M...   \n",
       "4      RUGE: O espera a ser devorado (Spanish Edition)   \n",
       "..                                                 ...   \n",
       "977           EL DR. SILKWORTH EN ALCOHÓLICOS ANÓNIMOS   \n",
       "978  La Inspiración de Rumi: 100 Citas Para Elevar ...   \n",
       "979  El Derecho como objeto de investigación: Enfoq...   \n",
       "980  Reflexiones jurídicas sobre la Jurisdicción Es...   \n",
       "981  El arte de no encajar: Lo que pasa cuando el a...   \n",
       "\n",
       "                              authors  \\\n",
       "0                        Agustin Laje   \n",
       "1     Prince Harry The Duke of Sussex   \n",
       "2                      Colleen Hoover   \n",
       "3                       Kushner Janet   \n",
       "4                        Daniel Habif   \n",
       "..                                ...   \n",
       "977         Oslos Molina Oslos Molina   \n",
       "978                       David Smith   \n",
       "979                  Elmer Arce Ortíz   \n",
       "980              Jaime Bernal Cuéllar   \n",
       "981          Noemí Navarro noemimisma   \n",
       "\n",
       "                                       publisher  \\\n",
       "0                           HarperCollins Mexico   \n",
       "1                            PRH Grupo Editorial   \n",
       "2                 Planeta Publishing Corporation   \n",
       "3                                       Larousse   \n",
       "4        Editorial Planeta Mexicana S.A. de C.V.   \n",
       "..                                           ...   \n",
       "977                              Experiencias AA   \n",
       "978                      Independently published   \n",
       "979                            Palestra Editores   \n",
       "980                        Universidad Externado   \n",
       "981  Penguin Random House Grupo Editorial España   \n",
       "\n",
       "                                                 blurb  \\\n",
       "0    El controversial escritor Agustín Laje adviert...   \n",
       "1    La controversial autobiografía del príncipe Ha...   \n",
       "2    ¡Colleen Hoover nos brinda su magia nuevamente...   \n",
       "3    Jauja, la popular chef estrella de YouTube, no...   \n",
       "4    Con su clásico agresivo pero muy alentador est...   \n",
       "..                                                 ...   \n",
       "977  Bill decía frecuentemente que el programa de A...   \n",
       "978  En este libro, te sumergirás en la sabiduría y...   \n",
       "979  Este libro es una presentación de la investiga...   \n",
       "980  El libro Reflexiones jurídicas sobre la Jurisd...   \n",
       "981  El libro testimonial de @Noemimisma Este es el...   \n",
       "\n",
       "                                             full_text  \n",
       "0    Generación idiota: Una crítica al adolescentri...  \n",
       "1    Spare: En la sombra PRH Grupo Editorial La con...  \n",
       "2    Volver a empezar / It Starts with Us (Spanish ...  \n",
       "3    ¡Vámonos a la Estufa! con Janet Jauja Cocina M...  \n",
       "4    RUGE: O espera a ser devorado (Spanish Edition...  \n",
       "..                                                 ...  \n",
       "977  EL DR. SILKWORTH EN ALCOHÓLICOS ANÓNIMOS Exper...  \n",
       "978  La Inspiración de Rumi: 100 Citas Para Elevar ...  \n",
       "979  El Derecho como objeto de investigación: Enfoq...  \n",
       "980  Reflexiones jurídicas sobre la Jurisdicción Es...  \n",
       "981  El arte de no encajar: Lo que pasa cuando el a...  \n",
       "\n",
       "[982 rows x 5 columns]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "books = pd.read_csv('/Users/javm/Desktop/Projects/Sistema-Recomendacion-Libros/books.csv')\n",
    "books.dropna(inplace = True)\n",
    "books['full_text'] = books['title'] + ' ' + books['publisher'] + ' ' + books['blurb']\n",
    "books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clean_and_lemmatize(text):\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove scraping notes from text\n",
    "    text = text.replace('spanish edition', '')\n",
    "    text = text.replace('overview\\nnotes from your bookseller\\n', '')\n",
    "    text = text.replace('overview\\n', '')\n",
    "    text = text.replace('\\n', ' ')\n",
    "    text = text.replace('@', '')\n",
    "\n",
    "    # Remove non-word characters (including punctuation) from text\n",
    "    pattern = r'[¡!¿?.,:;()\\-—«»“”‘’\\[\\]{}\\/\\'\\\"\\d]'\n",
    "    text = re.sub(pattern, '', text)\n",
    "\n",
    "    # Remove stop words from text\n",
    "    stop_words = set(stopwords.words(\"spanish\"))\n",
    "    text = \" \".join([word for word in nltk.word_tokenize(text) if word.lower() not in stop_words])\n",
    "\n",
    "    # Lemmatize text\n",
    "    nlp = spacy.load('es_core_news_sm')\n",
    "    doc = nlp(text)\n",
    "    text = ' '.join([token.lemma_ for token in doc])\n",
    "\n",
    "    # Return the cleaned and lemmatized text as a string\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "books['full_text'] = books['full_text'].apply(clean_and_lemmatize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Colleen Hoover'"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "books['authors'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_vec = TfidfVectorizer(min_df = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = books['full_text']\n",
    "vectors = tf_vec.fit_transform(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_df = pd.DataFrame(vectors.toarray(),\n",
    "                     columns=[k for k, v in sorted(tf_vec.vocabulary_.items(), \n",
    "                     key=lambda item: item[1])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aar</th>\n",
       "      <th>abajo</th>\n",
       "      <th>abandonado</th>\n",
       "      <th>abandonar</th>\n",
       "      <th>abandoned</th>\n",
       "      <th>abandono</th>\n",
       "      <th>abarca</th>\n",
       "      <th>abarcar</th>\n",
       "      <th>abc</th>\n",
       "      <th>abierto</th>\n",
       "      <th>...</th>\n",
       "      <th>íntimo</th>\n",
       "      <th>ón</th>\n",
       "      <th>óptimo</th>\n",
       "      <th>órden</th>\n",
       "      <th>órgano</th>\n",
       "      <th>último</th>\n",
       "      <th>únicamente</th>\n",
       "      <th>único</th>\n",
       "      <th>ús</th>\n",
       "      <th>útil</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.083708</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.056179</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.050798</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>977</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.047501</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>978</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>979</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.054782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>980</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>981</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>982 rows × 7216 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     aar  abajo  abandonado  abandonar  abandoned  abandono  abarca  abarcar  \\\n",
       "0    0.0    0.0         0.0        0.0        0.0       0.0     0.0      0.0   \n",
       "1    0.0    0.0         0.0        0.0        0.0       0.0     0.0      0.0   \n",
       "2    0.0    0.0         0.0        0.0        0.0       0.0     0.0      0.0   \n",
       "3    0.0    0.0         0.0        0.0        0.0       0.0     0.0      0.0   \n",
       "4    0.0    0.0         0.0        0.0        0.0       0.0     0.0      0.0   \n",
       "..   ...    ...         ...        ...        ...       ...     ...      ...   \n",
       "977  0.0    0.0         0.0        0.0        0.0       0.0     0.0      0.0   \n",
       "978  0.0    0.0         0.0        0.0        0.0       0.0     0.0      0.0   \n",
       "979  0.0    0.0         0.0        0.0        0.0       0.0     0.0      0.0   \n",
       "980  0.0    0.0         0.0        0.0        0.0       0.0     0.0      0.0   \n",
       "981  0.0    0.0         0.0        0.0        0.0       0.0     0.0      0.0   \n",
       "\n",
       "     abc   abierto  ...    íntimo   ón  óptimo  órden  órgano    último  \\\n",
       "0    0.0  0.000000  ...  0.000000  0.0     0.0    0.0     0.0  0.000000   \n",
       "1    0.0  0.083708  ...  0.000000  0.0     0.0    0.0     0.0  0.056179   \n",
       "2    0.0  0.000000  ...  0.000000  0.0     0.0    0.0     0.0  0.000000   \n",
       "3    0.0  0.000000  ...  0.050798  0.0     0.0    0.0     0.0  0.000000   \n",
       "4    0.0  0.000000  ...  0.000000  0.0     0.0    0.0     0.0  0.000000   \n",
       "..   ...       ...  ...       ...  ...     ...    ...     ...       ...   \n",
       "977  0.0  0.000000  ...  0.000000  0.0     0.0    0.0     0.0  0.047501   \n",
       "978  0.0  0.000000  ...  0.000000  0.0     0.0    0.0     0.0  0.000000   \n",
       "979  0.0  0.000000  ...  0.000000  0.0     0.0    0.0     0.0  0.000000   \n",
       "980  0.0  0.000000  ...  0.000000  0.0     0.0    0.0     0.0  0.000000   \n",
       "981  0.0  0.000000  ...  0.000000  0.0     0.0    0.0     0.0  0.000000   \n",
       "\n",
       "     únicamente  único   ús      útil  \n",
       "0           0.0    0.0  0.0  0.000000  \n",
       "1           0.0    0.0  0.0  0.000000  \n",
       "2           0.0    0.0  0.0  0.000000  \n",
       "3           0.0    0.0  0.0  0.000000  \n",
       "4           0.0    0.0  0.0  0.000000  \n",
       "..          ...    ...  ...       ...  \n",
       "977         0.0    0.0  0.0  0.000000  \n",
       "978         0.0    0.0  0.0  0.000000  \n",
       "979         0.0    0.0  0.0  0.054782  \n",
       "980         0.0    0.0  0.0  0.000000  \n",
       "981         0.0    0.0  0.0  0.000000  \n",
       "\n",
       "[982 rows x 7216 columns]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_search = input('What are you looking for: ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amigos juegos aventuras\n"
     ]
    }
   ],
   "source": [
    "print(book_search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "search = clean_and_lemmatize(book_search)\n",
    "search_vec = tf_vec.transform([search])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = cosine_similarity(search_vec, vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title : La Reina Druida: (El Sendero del Guardabosques, Libro 16)\n",
      "Author :  Pedro Urvi\n",
      "Publisher : Independently published\n",
      "Blurb : Las Panteras tendrán que proteger a la futura reina de Norghana. Una labor nada agradable y muy peligrosa.  Hay intereses politicos en juego y algunos reinos y facciones no ven la nueva alianza entre Norghana e Irinel con buenos ojos. Hay quienes quieren que la reina druida no alcance el trono.  Por si eso no fuera suficiente, otro peligro más importante acecha a Norghana y a todo Tremia. Un peligro de enormes dimensiones que amenaza con descender y arrasar reinos enteros.  Una nueva aventura épica aguarda a nuestros amigos. Una de muy dificil solución.  ¡Disfruta de unas aventuras llenas de acción, aventura, magia y romance!  Fantasía épica para toda la familia\n"
     ]
    }
   ],
   "source": [
    "best_book = np.argmax(test[0])\n",
    "\n",
    "print('Title : {}'.format(books.loc[best_book, 'title']))\n",
    "print('Author : {}'.format(books.loc[best_book, 'authors']))\n",
    "print('Publisher : {}'.format(books.loc[best_book, 'publisher']))\n",
    "print('Blurb : {}'.format(books.loc[best_book, 'blurb']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title : Diario de Junior, El\n",
      "Author :               James Patterson    Steven Butler    Richard Watson  \n",
      "Publisher : Duomo ediciones\n",
      "Blurb : ¡Finalmente puedo contar mi historia! Ser el perro de Rafe no siempre es fácil... ¡pero nunca es aburrido! Tengo mucho que contaros sobre: Cómo protejo el patio de pájaros, mapaches, ardillas, mapaches... ¿y mencioné a los…mapaches? La mejor manera de olfatear el trasero de un perro para descubrir las últimas noticias caninas. El monstruo aterrador del armario del pasillo: ¡la aspiradora! Pero lo peor de todo, la malvada Señora Stricker me mandará de nuevo a la perrera si no aprendo a comportarme. ¿Qué debe hacer un chucho como yo? I can finally tell my story! Being Rafe's dog isn't always easy... but it's never boring! I have a lot to tell you about: how I protect my yard from birds, raccoons, squirrels, raccoons… and did I mention… raccoons? The best way to sniff a dog's butt to find out the latest canine news. The scary monster in the hall closet: the vacuum cleaner! But worst of all, wicked Mrs. Stricker will send me back to the kennel if I don't learn to behave. What's a mutt like me to do?\n",
      "\n",
      "Title : Como Elegir un Gato\n",
      "Author :       Ana Martos  \n",
      "Publisher : Henari\n",
      "Blurb : Descubra la mascota perfecta para su familia con nuestro libro electrónico sobre cómo elegir a su gato. Esta completa guía ofrece consejos útiles para que usted y su gato formen una pareja perfecta. Desde saber qué tipo de gato se adapta mejor a su estilo de vida hasta aprender a cuidar adecuadamente de su nuevo amigo felino, nuestro ebook sobre cómo elegir su gato lo abarca todo. Si está buscando la mascota perfecta para su familia, no se pierda esta guía esencial. Consiga ya su ejemplar y aprenda a encontrar el gato adecuado para usted.\n",
      "\n",
      "Title : Aventuras en la bañera de Andrés el bebé y Mickey el gato\n",
      "Author :               Tom Haydon    Annika Haydon    Magdalena Zareba  Illustrator   \n",
      "Publisher : TomAnni Publishing\n",
      "Blurb : Estos son Andrés y Mickey, el duo más simpático y poco común de mejores amigos. Andrés es un bebé al que le encanta todo lo que tiene que ver con bañarse, desde las burbujas hasta el patito de goma. Mickey es un gato al que en realidad no le gusta bañarse, pero le encanta pasar tiempo con Andrés. ¡Juntos forman el equipo ¡juntos forman un equipo perfecto!  Este libro es una excelente opción como historia para acostar a los niños porque cuenta las divertidas aventuras en la bañera de dos amigos, Andrés el bebé y Mickey el gato. A pesar de que tienen gustos diferentes sobre el baño, ambos disfrutan pasar tiempo juntos y forman un \"equipo perfecto\". Con solo 25 páginas y un tamaño de 8x10, es una historia corta y adecuada para la hora de dormir que dejará a los niños con una sonrisa en el rostro y con ganas de tener sus propias aventuras con sus amigos. Además, el tamaño del libro es perfecto para que los niños lo sostengan y disfruten de la lectura antes de dormir.\n",
      "\n",
      "Title : Lamer las heridas (Lick the wounds - Spanish Edition)\n",
      "Author :       Leticia Castro  \n",
      "Publisher : HarperCollins Publishers\n",
      "Blurb : Un debut deslumbrante y conmovedor que nos reconcilia con el lado más puro del alma humana. La nueva revelación de la narrativa femenina que llega de Argentina.Un debut deslumbrante y conmovedor que nos reconcilia con el lado más puro del alma humana. La nueva revelación de la narrativa femenina que llega de Argentina.Un perro vagabundea por un pueblo del sur de España. Está lastimado y famélico. Camila se encuentra igual de perdida y dañada que él. Es argentina, tiene treinta y ocho años y acaba de abandonar su ciudad de origen, familia, trabajo, amigos y medicación psiquiátrica. Su huida la lleva a refugiarse en el pueblo andaluz donde se cruzará con el perro. Aunque a ella no le gustan los animales, decide ayudarlo: el perro tiene una herida infectada que requiere de cuidados y reposo. No sabemos qué fantasmas acompañan a Camila en su nueva vida en España, lo que sí sabemos es que no le permitirán establecer vínculos afectivos con las personas que allí encuentra. Solo la necesidad de ayuda del perro parece hacer mella en la coraza que trae de Buenos Aires. Esta es también la historia del perro, contada desde su perspectiva. Sentiremos su hambre, su soledad, su anhelo de calor humano. Y lo mejor: a través de él disfrutaremos de las cosas más sencillas, aquellas por las que merece la pena vivir.\n",
      "\n",
      "Title : EL MONJE QUE AMABA A LOS GATOS: Las siete revelaciones\n",
      "Author :           Corrado Debiasi    Manuel Manzano  Translator   \n",
      "Publisher : Editorial Sirio\n",
      "Blurb : Si alguien te dijera que debido a un extraño giro del destino vas a pasar un tiempo en compañía de un monje anciano y sus maravillosos gatos, ¿lo creerías?; que emprenderás un viaje iniciático, salpicado de encuentros que te llevarán a descubrir, a través de un torbellino de emociones, la inmensa belleza de tu alma, ¿lo creerías? Cuando el protagonista de esta historia, Kripala, emprende un viaje, no sabe qué le deparará el futuro, pero sabe lo que quiere dejar atrás. Su destino es la ciudad de Benarés (Varanasi) en la India. Allí se perderá en un laberinto de callejones en los que, paradójicamente, comenzará a encontrarse a sí mismo. En el vientre vital y sagrado de esa antigua ciudad se cruzará con personas extraordinarias en su aparente sencillez, humildes en su naturaleza, pero de sabiduría abismal. Un maestro de artes marciales, un pintor, una anciana que da de comer a los pobres, un jardinero… Todos ellos dejarán en Kripala enseñanzas imborrables, palabras que quedarán grabadas en su interior, pero será el anciano monje llamado Tatanji —aquel que teje destinos como un hábil tejedor—, retirado a un ashram en compañía de sus gatos, quien abrirá su alma para siempre.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Assume `test` is a numpy array containing the test results\n",
    "# and `books` is a pandas dataframe containing book data\n",
    "\n",
    "# Get the top k indices of the highest values in `test`\n",
    "k = 5\n",
    "top_k = np.argpartition(test[0], -k)[-k:]\n",
    "\n",
    "# Print information about each book\n",
    "for i in top_k:\n",
    "    print('Title : {}'.format(books.loc[i, 'title']))\n",
    "    print('Author : {}'.format(books.loc[i, 'authors']))\n",
    "    print('Publisher : {}'.format(books.loc[i, 'publisher']))\n",
    "    print('Blurb : {}'.format(books.loc[i, 'blurb']))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nlp = spacy.load('es_core_news_sm')\n",
    "\n",
    "# def clean_and_lemmatize(df, column_name):\n",
    "#     # Convert text to lowercase\n",
    "#     df[column_name] = df[column_name].str.lower()\n",
    "\n",
    "#     # Remove scraping notes from text\n",
    "#     df[column_name] = df[column_name].str.replace('spanish edition', '')\n",
    "#     df[column_name] = df[column_name].str.replace('overview\\nnotes from your bookseller\\n', '')\n",
    "#     df[column_name] = df[column_name].str.replace('overview\\n', '')\n",
    "#     df[column_name] = df[column_name].str.replace('\\n', ' ')\n",
    "#     df[column_name] = df[column_name].str.replace('@', '')\n",
    "\n",
    "#     # Remove non-word characters (including punctuation) from text\n",
    "#     pattern = r'[¡!¿?.,:;()\\-—«»“”‘’\\[\\]{}\\/\\'\\\"\\d]'\n",
    "#     df[column_name] = df[column_name].apply(lambda x: re.sub(pattern, '', str(x)))\n",
    "\n",
    "#     # Remove stop words from text\n",
    "#     stop_words = set(stopwords.words(\"spanish\"))\n",
    "#     df[column_name] = df[column_name].apply(lambda x: \" \".join([word for word in nltk.word_tokenize(x) if word.lower() not in stop_words]))\n",
    "\n",
    "#     # Lemmatize text\n",
    "#     df[column_name] = df[column_name].apply(lambda x: ' '.join([token.lemma_ for token in nlp(x)]))\n",
    "\n",
    "#     return df\n",
    "\n",
    "\n",
    "\n",
    "# DIFFICULT TO PROCESS INPUT TEXT\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import spacy\n",
    "# nlp = spacy.load('es_core_news_sm')\n",
    "\n",
    "# def spacy_tag_and_lemmatize(text_column):\n",
    "#     tagged_lemmatized = []\n",
    "#     for text in text_column:\n",
    "#         doc = nlp(text)\n",
    "#         tagged_lemmas = ' '.join([token.lemma_ + '_' + token.pos_ for token in doc])\n",
    "#         tagged_lemmatized.append(tagged_lemmas)\n",
    "#     return tagged_lemmatized\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
